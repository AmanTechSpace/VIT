{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx3pZykKkPXK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a configuration for the model using a data class\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 256          # Dimension of the model embeddings\n",
        "    hidden_dim: int = 512   # Dimension of the hidden layers\n",
        "    n_heads: int = 8        # Number of attention heads\n",
        "    n_layers: int = 6       # Number of layers in the transformer\n",
        "    patch_size: int = 4     # Size of the patches (typically square)\n",
        "    n_channels: int = 3     # Number of input channels (e.g., 3 for RGB images)\n",
        "    n_patches: int = 64     # Number of patches in the input\n",
        "    n_classes: int = 2     # Number of target classes   -2\n",
        "    dropout: float = 0.2    # Dropout rate for regularization\n"
      ],
      "metadata": {
        "id": "Zjl3afh_kQ9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        # Linear projections for Q, K, and V\n",
        "        self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, seq_len, dim = x.shape  # b: batch size, seq_len: sequence length\n",
        "\n",
        "        assert dim == self.dim, \"dim is not matching\"\n",
        "\n",
        "        q = self.wq(x)  # [b, seq_len, n_heads*head_dim]\n",
        "        k = self.wk(x)  # [b, seq_len, n_heads*head_dim]\n",
        "        v = self.wv(x)  # [b, seq_len, n_heads*head_dim]\n",
        "\n",
        "        # Reshape the tensors for multi-head operations\n",
        "        q = q.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
        "        k = k.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
        "        v = v.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
        "\n",
        "        # Transpose to bring the head dimension to the front\n",
        "        q = q.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
        "        k = k.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
        "        v = v.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
        "\n",
        "        # Compute attention scores and apply softmax\n",
        "        attn = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)  # [b, n_heads, seq_len, seq_len]\n",
        "        attn_scores = F.softmax(attn, dim=-1)  # [b, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # Compute the attended features\n",
        "        out = torch.matmul(attn_scores, v)  # [b, n_heads, seq_len, head_dim]\n",
        "        out = out.contiguous().view(b, seq_len, -1)  # [b, seq_len, n_heads*head_dim]\n",
        "\n",
        "        return self.wo(out), attn_scores  # Return both output and attention scores\n"
      ],
      "metadata": {
        "id": "QOwserKYl_gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = nn.LayerNorm(args.dim)\n",
        "        self.attn = MultiHeadAttention(args)\n",
        "\n",
        "        self.layer_norm_2 = nn.LayerNorm(args.dim)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(args.dim, args.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(args.dropout),\n",
        "            nn.Linear(args.hidden_dim, args.dim),\n",
        "            nn.Dropout(args.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_weights = self.attn(self.layer_norm_1(x))\n",
        "        x = x + attn_weights[0]  # Adding the attention weights output\n",
        "        x_ffn = self.ffn(self.layer_norm_2(x))\n",
        "        x = x + x_ffn\n",
        "        return x, attn_weights[1]  # Returning both the output and attention weights\n"
      ],
      "metadata": {
        "id": "IUC-T4iGmZx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    # x: Input image tensor\n",
        "    # B: Batch size, C: Channels, H: Height, W: Width\n",
        "    B, C, H, W = x.shape  # (B, C, H, W)\n",
        "\n",
        "    # Reshape the image tensor to get non-overlapping patches\n",
        "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)  # (B, C, H/patch_size, patch_size, W/patch_size, patch_size)\n",
        "\n",
        "    # Permute to group the patches and channels\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5)  # (B, H/patch_size, W/patch_size, C, patch_size, patch_size)\n",
        "\n",
        "    # Flatten the height and width dimensions for patches\n",
        "    x = x.flatten(1,2)  # (B, (H/patch_size * W/patch_size), C, patch_size, patch_size)\n",
        "\n",
        "    # Option to flatten the channel and spatial dimensions\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2,4)  # (B, (H/patch_size * W/patch_size), (C * patch_size * patch_size))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "bIoCSnnWmmfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the patch size\n",
        "        self.patch_size = args.patch_size\n",
        "\n",
        "        # Embedding layer to transform flattened patches to the desired dimension\n",
        "        self.input_layer = nn.Linear(args.n_channels * (args.patch_size ** 2), args.dim)\n",
        "\n",
        "        # Create the attention blocks for the transformer\n",
        "        attn_blocks = []\n",
        "        for _ in range(args.n_layers):\n",
        "            attn_blocks.append(AttentionBlock(args))\n",
        "\n",
        "        # Create the transformer by stacking the attention blocks\n",
        "        self.transformer = nn.Sequential(*attn_blocks)\n",
        "\n",
        "        # Define the classifier\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.LayerNorm(args.dim),\n",
        "            nn.Linear(args.dim, args.n_classes)\n",
        "        )\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "\n",
        "        # Define the class token (similar to BERT's [CLS] token)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, args.dim))\n",
        "\n",
        "        # Positional embeddings to give positional information to the transformer\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + args.n_patches, args.dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert image to patches and flatten\n",
        "        x_patches = img_to_patch(x, self.patch_size)\n",
        "        b, seq_len, _ = x_patches.shape\n",
        "\n",
        "        # Transform patches using the embedding layer\n",
        "        x = self.input_layer(x_patches)\n",
        "\n",
        "        # Add the class token to the beginning of each sequence\n",
        "        cls_token = self.cls_token.repeat(b, 1, 1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "\n",
        "        # Add positional embeddings to the sequence\n",
        "        x = x + self.pos_embedding[:, :seq_len + 1]\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Process sequence through the transformer, capturing attention weights\n",
        "        attn_weights = []\n",
        "        for block in self.transformer:\n",
        "            x, attn_weight = block(x)\n",
        "            attn_weights.append(attn_weight)\n",
        "\n",
        "        # Retrieve the class token's representation (for classification)\n",
        "        x = x.transpose(0, 1)\n",
        "        cls = x[0]\n",
        "\n",
        "        # Classify using the representation of the class token\n",
        "        out = self.mlp(cls)\n",
        "        return out, attn_weights\n"
      ],
      "metadata": {
        "id": "NeYnyzPMmrBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the directory where CIFAR10 data will be stored/downloaded\n",
        "DATA_DIR = \"../data\"\n",
        "\n",
        "# Define the transformation for testing dataset:\n",
        "# 1. Convert images to tensors.\n",
        "# 2. Normalize the tensors using the mean and standard deviation of CIFAR10 dataset.\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "])\n",
        "\n",
        "# Define the transformation for training dataset:\n",
        "# 1. Apply random horizontal flip for data augmentation.\n",
        "# 2. Perform random resizing and cropping of images for data augmentation.\n",
        "# 3. Convert images to tensors.\n",
        "# 4. Normalize the tensors using the mean and standard deviation of CIFAR10 dataset.\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "])\n",
        "\n",
        "# Load the CIFAR10 training dataset with the defined training transformation.\n",
        "# The dataset will be downloaded if not present in the DATA_DIR.\n",
        "train_dataset = CIFAR10(root=DATA_DIR, train=True, transform=train_transform, download=True)\n",
        "\n",
        "# Load the CIFAR10 testing dataset with the defined testing transformation.\n",
        "# The dataset will be downloaded if not present in the DATA_DIR.\n",
        "test_set = CIFAR10(root=DATA_DIR, train=False, transform=test_transform, download=True)\n",
        "\n",
        "# Split the training dataset into training and validation sets.\n",
        "# The training set will have 45000 images, and the validation set will have 5000 images.\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvCSxdq8qfDJ",
        "outputId": "b4b19b2c-eba7-4b65-d4f5-0121cc1d5ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48531066.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size for training, validation, and testing.\n",
        "batch_size = 64\n",
        "\n",
        "# Define the number of subprocesses to use for data loading.\n",
        "num_workers = 16\n",
        "\n",
        "# Create a DataLoader for the training and validation dataset:\n",
        "# 1. Shuffle the training data for each epoch.\n",
        "# 2. Drop the last batch if its size is not equal to `batch_size` to maintain consistency.\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=num_workers,\n",
        "                                           drop_last=True)\n",
        "\n",
        "# Do not drop any data; process all the validation data.\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=num_workers,\n",
        "                                         drop_last=False)\n",
        "\n",
        "# Create a DataLoader for the testing dataset:\n",
        "# Do not drop any data; process all the test data.\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=num_workers,\n",
        "                                          drop_last=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FAYVE8Sq-Xc",
        "outputId": "af1bdce0-1bc2-4719-844a-f3f6f76166be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Loss and Optimizer\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else 0\n",
        "args = ModelArgs()\n",
        "model = VisionTransformer(args).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 130], gamma=0.1)"
      ],
      "metadata": {
        "id": "D3ZBh92WrIB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, train_loader, val_loader, device, lr_scheduler, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs,_ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs,_ = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(dim=-1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "# To use this function:\n",
        "train(model, criterion, optimizer, train_loader, val_loader, device, lr_scheduler, num_epochs=10)\n",
        "# I  have set epochs=10 only due to limited time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piTEf5CcrNEP",
        "outputId": "f2630206-0a4e-4a0e-bc7d-66e93d7ac546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 1.4116\n",
            "Epoch [1/10], Validation Loss: 1.3596, Validation Accuracy: 50.88%\n",
            "Epoch [2/10], Training Loss: 1.3060\n",
            "Epoch [2/10], Validation Loss: 1.2757, Validation Accuracy: 53.82%\n",
            "Epoch [3/10], Training Loss: 1.2396\n",
            "Epoch [3/10], Validation Loss: 1.1979, Validation Accuracy: 57.24%\n",
            "Epoch [4/10], Training Loss: 1.1857\n",
            "Epoch [4/10], Validation Loss: 1.2308, Validation Accuracy: 56.66%\n",
            "Epoch [5/10], Training Loss: 1.1428\n",
            "Epoch [5/10], Validation Loss: 1.1661, Validation Accuracy: 58.76%\n",
            "Epoch [6/10], Training Loss: 1.1092\n",
            "Epoch [6/10], Validation Loss: 1.1597, Validation Accuracy: 59.34%\n",
            "Epoch [7/10], Training Loss: 1.0796\n",
            "Epoch [7/10], Validation Loss: 1.0899, Validation Accuracy: 61.56%\n",
            "Epoch [8/10], Training Loss: 1.0443\n",
            "Epoch [8/10], Validation Loss: 1.1017, Validation Accuracy: 60.42%\n",
            "Epoch [9/10], Training Loss: 1.0181\n",
            "Epoch [9/10], Validation Loss: 1.1031, Validation Accuracy: 60.36%\n",
            "Epoch [10/10], Training Loss: 0.9953\n",
            "Epoch [10/10], Validation Loss: 1.0888, Validation Accuracy: 61.40%\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs,_ = model(inputs)\n",
        "        _, predicted = outputs.max(dim=-1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEnVZN_swaPq",
        "outputId": "aa55a7d5-abfb-4e70-8cd1-0b28acffe16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 63.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the state dictionary of the model\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n"
      ],
      "metadata": {
        "id": "kV32z2tM8TPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1O6b-LuU9QFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-Q4ZaWa9RnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emoViyWu9TFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDkIYVob9U1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U9UklN2E9Xgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCPJl8329hWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vfu9unz9khQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JyLJTSUn9mUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "caKvYJ6l_aL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1QxxCLXZ_ffS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HA91RpmCMFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}